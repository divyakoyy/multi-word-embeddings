{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_file(filepath, lines=1000000):\n",
    "    \"\"\"Split a file based on a number of lines.\"\"\"\n",
    "    path, filename = os.path.split(filepath)\n",
    "    # filename.split('.') would not work for filenames with more than one .\n",
    "    basename, ext = os.path.splitext(filename)\n",
    "    # open input file\n",
    "    with open(filepath, 'r') as f_in:\n",
    "        try:\n",
    "            # open the first output file\n",
    "            f_out = open(os.path.join(path, '{}_{}{}'.format(basename, 0, ext)), 'w')\n",
    "            # loop over all lines in the input file, and number them\n",
    "            for i, line in enumerate(f_in):\n",
    "                # every time the current line number can be divided by the\n",
    "                # wanted number of lines, close the output file and open a\n",
    "                # new one\n",
    "                if i % lines == 0:\n",
    "                    f_out.close()\n",
    "                    f_out = open(os.path.join(path, '{}_{}{}'.format(basename, i, ext)), 'w')\n",
    "                # write the line to the output file\n",
    "                f_out.write(line)\n",
    "        finally:\n",
    "            # close the last output file\n",
    "            f_out.close()\n",
    "split_file('data/WestburyLab.Wikipedia.Corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### CLEAR TEST FILES ###\n",
    "########################\n",
    "open('data/test/1.txt', 'w').close()\n",
    "open('data/test/df.txt', 'w').close()\n",
    "open('data/test/vocab.txt', 'w').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## POS tagging\n",
    "* to build our vocabulary, we need to build a set of words that is word_POS. then we can map these to word ids set up like: numword_numpos \n",
    "* keep count of number of documents\n",
    "* keep track of how many documents contain a given word\n",
    "NOTE: end of text file doesn't always end a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### INITIALIZE VARIABLES ###\n",
    "############################\n",
    "\n",
    "# # set up doc freq file\n",
    "# import in_place\n",
    "# with in_place.InPlace('data/df_pos.txt') as df:\n",
    "#     df.write(str(0))\n",
    "\n",
    "# see https://spacy.io/api/annotation#pos-tagging for description of POS\n",
    "\n",
    "POS_DICT = {\n",
    "    'NOUN': 0,\n",
    "    'VERB': 1,\n",
    "    'ADJ': 2,\n",
    "    'ADV': 3, \n",
    "    'DET': 4, \n",
    "    'CONJ': 5,  \n",
    "    'NUM': 6, \n",
    "    'ADP': 7, \n",
    "    'PROPN': 8, \n",
    "    'PART': 9, \n",
    "    'SPACE': 10, \n",
    "    'INTJ': 11, \n",
    "    'PRON': 12,\n",
    "    'SYM': 14,     \n",
    "    'X': 15\n",
    "}\n",
    "\n",
    "WORD_ID = 'word_id'\n",
    "FREQUENCY = 'frequency'\n",
    "NUM_DOCS = 'num_docs'\n",
    "SEEN_IN_DOC = 'seen_in_doc'\n",
    "LAST_WORD_ID = 2 # 0, 1 and 2 used for eof, beginning padding and end padding\n",
    "OTHER_POS = set()\n",
    "\n",
    "# the n-th value of df_vals is the number of documents that contain the word with id n-1\n",
    "df_vals = []    \n",
    "\n",
    "# a set of all the words (e.g. duck, run, etc.). used to look up if a new word_id should be created\n",
    "words = set()\n",
    "\n",
    "# vocab words mapped to their [word_id, frequency, num_docs, seen_in_doc] in the corpus. each vocab word is the originalword_pos (e.g. duck_noun and duck_verb)\n",
    "# word_id is a string formatted as \n",
    "vocab = dict()\n",
    "\n",
    "EOD = 'eeeoddd_x'\n",
    "BEG_PAD = '<s>_x'\n",
    "END_PAD = '</s>_x'\n",
    "WIKI_EOD = \"---END.OF.DOCUMENT---\"\n",
    "TOTAL_DOCS = 0\n",
    "\n",
    "import collections\n",
    "vocab = collections.OrderedDict()\n",
    "vocab[EOD] = {\n",
    "    WORD_ID:'1_x', \n",
    "    FREQUENCY: 0,\n",
    "    NUM_DOCS: 0,\n",
    "    SEEN_IN_DOC: False\n",
    "}\n",
    "vocab[BEG_PAD] = {\n",
    "    WORD_ID:'2_x', \n",
    "    FREQUENCY: 0,\n",
    "    NUM_DOCS: 0,\n",
    "    SEEN_IN_DOC: False\n",
    "}\n",
    "vocab[END_PAD] = {\n",
    "    WORD_ID:'3_x', \n",
    "    FREQUENCY: 0,\n",
    "    NUM_DOCS: 0,\n",
    "    SEEN_IN_DOC: False\n",
    "}\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### HELPER FUNCTIONS ###\n",
    "########################\n",
    "\n",
    "\n",
    "def write_to_file(fn, data):\n",
    "    with open(fn, 'a') as mfile:\n",
    "        mfile.write(data + '\\n')\n",
    "        \n",
    "def set_up_word(text, pos, vocab):\n",
    "    global LAST_WORD_ID, WORD_ID, POS_DICT, FREQUENCY, NUM_DOCS, SEEN_IN_DOC, TOTAL_DOCS\n",
    "    # TODO: why are some pos not in POS_DICT\n",
    "    if pos in POS_DICT:\n",
    "        LAST_WORD_ID += 1\n",
    "        vocab[word] = dict()\n",
    "        word_dict = vocab[word]\n",
    "        word_dict[WORD_ID] = str(LAST_WORD_ID) + \"_\" + str(POS_DICT[pos])\n",
    "        word_dict[FREQUENCY] = 1\n",
    "        word_dict[NUM_DOCS] = 1\n",
    "        word_dict[SEEN_IN_DOC] = True\n",
    "        return word_dict[WORD_ID]\n",
    "    else:\n",
    "        OTHER_POS.add(pos)\n",
    "        return None\n",
    "\n",
    "def update_word(text, pos, vocab):\n",
    "    global LAST_WORD_ID, WORD_ID, POS_DICT, FREQUENCY, NUM_DOCS, SEEN_IN_DOC, TOTAL_DOCS\n",
    "    word_dict = vocab[word]\n",
    "    word_dict[FREQUENCY] += 1\n",
    "    if not word_dict[SEEN_IN_DOC]:\n",
    "        word_dict[NUM_DOCS] += 1\n",
    "        word_dict[SEEN_IN_DOC] = True\n",
    "    return word_dict[WORD_ID]\n",
    "\n",
    "def update_eod(words_in_doc):\n",
    "    global LAST_WORD_ID, WORD_ID, POS_DICT, FREQUENCY, NUM_DOCS, SEEN_IN_DOC, TOTAL_DOCS\n",
    "    # set all SEEN_IN_DOC values to False for all words in the previous document\n",
    "    print \"updating EOD\"\n",
    "    for word in words_in_doc:\n",
    "        vocab[word][SEEN_IN_DOC] = False\n",
    "    # end padding\n",
    "    write_to_file('data/test/1.txt', str(2))\n",
    "    # end of document\n",
    "    write_to_file('data/test/1.txt', str(1))\n",
    "    # update frequencies\n",
    "    vocab[BEG_PAD][FREQUENCY] += 1\n",
    "    vocab[EOD][FREQUENCY] += 1\n",
    "    vocab[END_PAD][FREQUENCY] += 1\n",
    "    TOTAL_DOCS += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "#os.path.join('data/split_corpus_files/','WestburyLab.Wikipedia.Corpus_1000000.txt')\n",
    "\n",
    "import in_place\n",
    "\n",
    "######################\n",
    "### RUN EVERYTHING ###\n",
    "######################\n",
    "\n",
    "# write the beginning padding\n",
    "write_to_file('data/test/1.txt', str(1))\n",
    "\n",
    "with open('data/test/corpus/WestburyLab.Wikipedia.Corpus_1000000.txt', 'r') as corpus:\n",
    "    words_in_doc = set()\n",
    "    for line in corpus:\n",
    "       \n",
    "        if line.strip() == WIKI_EOD:\n",
    "            update_eod(words_in_doc)\n",
    "            continue\n",
    "        sentence = nlp(unicode(line.strip()), \"utf-8\")\n",
    "\n",
    "        for token in sentence:\n",
    "            text = token.text\n",
    "            pos = token.pos_\n",
    "            word = text.lower() + '_' + pos.lower()\n",
    "            if word not in vocab:\n",
    "                word_id = set_up_word(text, pos, vocab)\n",
    "            else:\n",
    "                word_id = update_word(text, pos, vocab)\n",
    "            if word_id != None: \n",
    "                words_in_doc.add(word)\n",
    "                write_to_file('data/test/1.txt', str(word_id))\n",
    "\n",
    "                \n",
    "                \n",
    "print TOTAL_DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### WRITE VOCAB FILE ###\n",
    "########################\n",
    "\n",
    "for word in vocab:\n",
    "    info = str(word) + \" \" + str(vocab[word][WORD_ID]) + \" \" + str(vocab[word][FREQUENCY])\n",
    "    write_to_file('data/test/vocab.txt', info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### WRITE DF FILE ###\n",
    "########################\n",
    "\n",
    "global TOTAL_DOCS\n",
    "write_to_file('data/test/df.txt', str(TOTAL_DOCS))\n",
    "    \n",
    "for word in vocab:\n",
    "    write_to_file('data/test/df.txt', str(vocab[word][NUM_DOCS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print TOTAL_DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
